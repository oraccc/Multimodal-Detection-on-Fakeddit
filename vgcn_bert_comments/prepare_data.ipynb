{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15d1fa3",
   "metadata": {},
   "source": [
    "### Step 1: Load Packages and Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from math import log\n",
    "import pickle as pkl\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Config\n",
    "DATASET_NUM_CLASSES = \"3\"\n",
    "DATASET_SIZE = \"small\"\n",
    "DATASET_BASE_DIR = \"../data/processed_data/\" + DATASET_NUM_CLASSES + \"_\" + DATASET_SIZE\n",
    "DATA_TYPE = \"text_comments\" \n",
    "DATA_SAVE_DIR = \"./processed_data/\" + DATASET_NUM_CLASSES + \"_\" + DATASET_SIZE + \"/\" + DATA_TYPE\n",
    "DUMP_DIR = DATA_SAVE_DIR + \"/dumped_data\"\n",
    "\n",
    "# BERT Tokenizer Config\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "# Graph Config\n",
    "MIN_FREQ = 5\n",
    "USE_STOPWORDS = False\n",
    "TFIDF_MODE = \"all_tfidf\" # \"only_tf\", \"all_tfidf\"\n",
    "WINDOW_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa527",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATA_SAVE_DIR):\n",
    "    os.makedirs(DATA_SAVE_DIR)\n",
    "    \n",
    "if not os.path.exists(DUMP_DIR):\n",
    "    os.makedirs(DUMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv(DATASET_BASE_DIR + \"/train_data.csv\")\n",
    "test_data_raw = pd.read_csv(DATASET_BASE_DIR + \"/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_raw[[DATA_TYPE, DATASET_NUM_CLASSES + \"_way_label\"]]\n",
    "test_data = test_data_raw[[DATA_TYPE, DATASET_NUM_CLASSES + \"_way_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns = {DATA_TYPE : \"text\", DATASET_NUM_CLASSES + \"_way_label\" : \"label\"}, inplace = True)\n",
    "test_data.rename(columns = {DATA_TYPE : \"text\", DATASET_NUM_CLASSES + \"_way_label\" : \"label\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = shuffle(train_data)\n",
    "test_data = shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-channel",
   "metadata": {},
   "source": [
    "### Step 2: Remove Stopwords and Rare Words and Get Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# to remove stopwords\n",
    "if USE_STOPWORDS:\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = set(stop_words)\n",
    "else: \n",
    "    stop_words = {}\n",
    "\n",
    "# to remove rare words\n",
    "word_freq = {}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_words_list = []\n",
    "\n",
    "for idx, row in tqdm(train_data.iterrows(), total = train_data.shape[0], desc=\"Tokenize Train Texts\", colour='green'):\n",
    "    sub_words = bert_tokenizer.tokenize(row[\"text\"])\n",
    "    sub_words_list.append(sub_words)\n",
    "    for word in sub_words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "            \n",
    "for idx, row in tqdm(test_data.iterrows(), total = test_data.shape[0], desc=\"Tokenize Test Texts\", colour='green'):\n",
    "    sub_words = bert_tokenizer.tokenize(row[\"text\"])\n",
    "    sub_words_list.append(sub_words)\n",
    "    for word in sub_words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens_list = []\n",
    "count_void_text = 0\n",
    "\n",
    "for idx, sub_words in enumerate(sub_words_list):\n",
    "    cleaned_sub_words = []\n",
    "\n",
    "    for word in sub_words:\n",
    "        if word not in stop_words and word_freq[word] >= MIN_FREQ:\n",
    "            cleaned_sub_words.append(word)\n",
    "\n",
    "    cleaned_tokens = \" \".join(cleaned_sub_words).strip()\n",
    "\n",
    "    if cleaned_tokens == \"\":\n",
    "        count_void_text += 1\n",
    "\n",
    "    cleaned_tokens_list.append(cleaned_tokens)\n",
    "    \n",
    "print(\"Total\", str(count_void_text), \"empty texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 1000000\n",
    "max_len = 0\n",
    "aver_len = 0\n",
    "\n",
    "for idx, line in enumerate(cleaned_tokens_list):\n",
    "    tokens = line.strip().split()\n",
    "    aver_len = aver_len + len(tokens)\n",
    "    if len(tokens) < min_len:\n",
    "        min_len = len(tokens)\n",
    "    if len(tokens) > max_len:\n",
    "        max_len = len(tokens)\n",
    "\n",
    "aver_len = 1.0 * aver_len / len(cleaned_tokens_list)\n",
    "print('Statistics after stopwords and tokenizer:')\n",
    "print('min_len : ' + str(min_len))\n",
    "print('max_len : ' + str(max_len))\n",
    "print('average_len : ' + str(aver_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-sender",
   "metadata": {},
   "source": [
    "### Step 3: Save Cleaned Tokens into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaned_tokens_list = cleaned_tokens_list[: len(train_data)]\n",
    "test_cleaned_tokens_list = cleaned_tokens_list[len(train_data) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_cleaned_tokens_list), len(test_cleaned_tokens_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data = train_data\n",
    "processed_train_data[\"cleaned_tokens\"] = train_cleaned_tokens_list\n",
    "\n",
    "processed_test_data = test_data\n",
    "processed_test_data[\"cleaned_tokens\"] = test_cleaned_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data.to_csv(DATA_SAVE_DIR + \"/processed_train_data.csv\")\n",
    "processed_test_data.to_csv(DATA_SAVE_DIR + \"/processed_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-triumph",
   "metadata": {},
   "source": [
    "### Step 4: Build Vocab Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "for cleaned_tokens in cleaned_tokens_list:\n",
    "    words = cleaned_tokens.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_map: mapping \"token\" to \"id\"\n",
    "\n",
    "vocab_map = {}\n",
    "for i in range(vocab_size):\n",
    "    vocab_map[vocab[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-desktop",
   "metadata": {},
   "source": [
    "#### 4.1: Calculate Word Freq in All Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_texts_map: mapping \"words\" to \"text_ids\"\n",
    "\n",
    "words_texts_map = {}\n",
    "\n",
    "for idx in range(len(cleaned_tokens_list)):\n",
    "    cleaned_tokens = cleaned_tokens_list[idx]\n",
    "    words = cleaned_tokens.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in words_texts_map:\n",
    "            text_ids = words_texts_map[word]\n",
    "            text_ids.append(idx)\n",
    "            words_texts_map[word] = text_ids\n",
    "        else:\n",
    "            words_texts_map[word] = [idx]\n",
    "        appeared.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_texts_freq: mapping \"words\" to \"appeared_in_all_texts_freq\"\n",
    "\n",
    "words_texts_freq = {}\n",
    "for word, text_ids in words_texts_map.items():\n",
    "    words_texts_freq[word] = len(text_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-monaco",
   "metadata": {},
   "source": [
    "#### 4.2 Build Windows and Calulate Word Freq within Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_list = []\n",
    "for cleaned_tokens in cleaned_tokens_list:\n",
    "    words = cleaned_tokens.split()\n",
    "    length = len(words)\n",
    "    if length <= WINDOW_SIZE:\n",
    "        window_list.append(words)\n",
    "    else:\n",
    "        window_list.append(words[:WINDOW_SIZE])\n",
    "\n",
    "print('cleaned_tokens size:', len(cleaned_tokens_list), ', window number:', len(window_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_windows_freq: mapping \"words\" to \"appeared_in_windows_freq\"\n",
    "\n",
    "words_windows_freq = {}\n",
    "\n",
    "for window in window_list:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in words_windows_freq:\n",
    "            words_windows_freq[window[i]] += 1\n",
    "        else:\n",
    "            words_windows_freq[window[i]] = 1\n",
    "        appeared.add(window[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_pair_freq: mapping \"word_pair\" to \"in-windows_freq\"\n",
    "\n",
    "word_pair_freq = {}\n",
    "\n",
    "for window in tqdm(window_list, desc=\"Word Cooccurence within Windows\", colour='green'):\n",
    "    appeared = set()\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = vocab_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = vocab_map[word_j]\n",
    "\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in appeared:\n",
    "                continue\n",
    "            if word_pair_str in word_pair_freq:\n",
    "                word_pair_freq[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_freq[word_pair_str] = 1\n",
    "            appeared.add(word_pair_str)\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in appeared:\n",
    "                continue\n",
    "            if word_pair_str in word_pair_freq:\n",
    "                word_pair_freq[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_freq[word_pair_str] = 1\n",
    "            appeared.add(word_pair_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-retailer",
   "metadata": {},
   "source": [
    "#### 4.3: Calculate PMI and NPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_row = []\n",
    "tfidf_col = []\n",
    "tfidf_weight = []\n",
    "vocab_adj_row = []\n",
    "vocab_adj_col = []\n",
    "vocab_adj_weight = []\n",
    "\n",
    "num_windows = len(window_list)\n",
    "max_npmi = 0\n",
    "min_npmi = 0\n",
    "max_pmi = 0\n",
    "min_pmi = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(word_pair_freq, desc=\"Calulating PMI and NPMI: \", colour='green'):\n",
    "    temp_key = key.split(',')\n",
    "    i = int(temp_key[0])\n",
    "    j = int(temp_key[1])\n",
    "    count = word_pair_freq[key]\n",
    "    word_freq_i = words_windows_freq[vocab[i]]\n",
    "    word_freq_j = words_windows_freq[vocab[j]]\n",
    "\n",
    "    pmi = log((1.0 * count / num_windows) /\n",
    "              (1.0 * word_freq_i * word_freq_j / (num_windows * num_windows)))\n",
    "\n",
    "    npmi = log(1.0 * word_freq_i * word_freq_j / \n",
    "               (num_windows * num_windows)) / log(1.0 * count / num_windows) - 1\n",
    "\n",
    "    if npmi > max_npmi:\n",
    "        max_npmi = npmi\n",
    "    if npmi < min_npmi:\n",
    "        min_npmi = npmi\n",
    "    if pmi > max_pmi:\n",
    "        max_pmi = pmi\n",
    "    if pmi < min_pmi:\n",
    "        min_pmi = pmi\n",
    "    if npmi > 0:\n",
    "        vocab_adj_row.append(i)\n",
    "        vocab_adj_col.append(j)\n",
    "        vocab_adj_weight.append(npmi)\n",
    "\n",
    "print('max_pmi:', max_pmi, 'min_pmi:', min_pmi)\n",
    "print('max_npmi:', max_npmi, 'min_npmi:', min_npmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-forth",
   "metadata": {},
   "source": [
    "#### 4.4: Calulate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_texts = len(cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_pair_freq: mapping \"word_text_pair\" to \"freq\"\n",
    "\n",
    "word_text_pair_freq = {}\n",
    "for text_id in range(num_texts):\n",
    "    cleaned_tokens = cleaned_tokens_list[text_id]\n",
    "    words = cleaned_tokens.split()\n",
    "    for word in words:\n",
    "        word_id = vocab_map[word]\n",
    "        text_word_str = str(text_id) + ',' + str(word_id)\n",
    "        \n",
    "        if text_word_str in word_text_pair_freq:\n",
    "            word_text_pair_freq[text_word_str] += 1\n",
    "        else:\n",
    "            word_text_pair_freq[text_word_str] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_texts):\n",
    "    cleaned_tokens = cleaned_tokens_list[i]\n",
    "    words = cleaned_tokens.split()\n",
    "    text_word_set = set()\n",
    "    tfidf_vec = []\n",
    "    for word in words:\n",
    "        if word in text_word_set:\n",
    "            continue\n",
    "        j = vocab_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        \n",
    "        tfidf_row.append(i)\n",
    "        tfidf_col.append(j)\n",
    "        \n",
    "        tf = word_text_pair_freq[key] \n",
    "        idf = log((1.0 + num_texts) / (1.0 + words_texts_freq[vocab[j]])) + 1.0\n",
    "        \n",
    "        tfidf_vec.append(tf * idf)\n",
    "        text_word_set.add(word)\n",
    "        \n",
    "    if len(tfidf_vec) > 0:\n",
    "        tfidf_weight.extend(tfidf_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-mambo",
   "metadata": {},
   "source": [
    "### Step 5: Assemble Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_adj_npmi = sp.csr_matrix((vocab_adj_weight, (vocab_adj_row, vocab_adj_col)), shape=(vocab_size, vocab_size), dtype=np.float32)\n",
    "vocab_adj_npmi.setdiag(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_all = sp.csr_matrix((tfidf_weight, (tfidf_row, tfidf_col)), shape=(num_texts, vocab_size), dtype=np.float32)\n",
    "\n",
    "vocab_tfidf = tfidf_all.T\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    norm = np.linalg.norm(vocab_tfidf.data[i])\n",
    "    if norm > 0:\n",
    "        vocab_tfidf.data[i] /= norm\n",
    "        \n",
    "vocab_adj_tf = vocab_tfidf.dot(vocab_tfidf.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-container",
   "metadata": {},
   "source": [
    "### Step 6: Dump Vocab Graph File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DUMP_DIR + \"/data.vocab_map\", 'wb') as f:\n",
    "    pkl.dump(vocab_map, f)\n",
    "with open(DUMP_DIR + \"/data.vocab_adj_npmi\", 'wb') as f:\n",
    "    pkl.dump(vocab_adj_npmi, f)\n",
    "with open(DUMP_DIR + \"/data.vocab_adj_tf\", 'wb') as f:\n",
    "    pkl.dump(vocab_adj_tf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-essex",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumour",
   "language": "python",
   "name": "rumour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
