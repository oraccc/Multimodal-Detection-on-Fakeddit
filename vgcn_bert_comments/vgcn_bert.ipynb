{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_BERT\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Config\n",
    "DATASET_NUM_CLASSES = \"3\"\n",
    "DATASET_SIZE = \"small\"\n",
    "DATA_TYPE = \"text_comments\"\n",
    "DATA_DIR = \"./processed_data/\" + DATASET_NUM_CLASSES + \"_\" + DATASET_SIZE + \"/\" + DATA_TYPE\n",
    "DUMP_DIR = DATA_DIR + \"/dumped_data\"\n",
    "\n",
    "# BERT Config\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "# Model Config\n",
    "GCN_EMBEDDING_DIM = 16\n",
    "LEARNING_RATE = 1e-5\n",
    "L2_DECAY = 0.01\n",
    "VOCAB_ADJ = \"npmi\" # npmi / tf / all\n",
    "NPMI_THRESHOLD = 0.1\n",
    "TF_THRESHOLD = 0.1\n",
    "MAX_SEQ_LENGTH = 450 + GCN_EMBEDDING_DIM\n",
    "TRAIN_EPOCH = 5\n",
    "BATCH_SIZE = 8\n",
    "OUTPUT_FILE = \"./trained_models/\" + DATASET_NUM_CLASSES + \"_\" + DATASET_SIZE\n",
    "MODEL_FILE = OUTPUT_FILE + \"/vgcn_bert_models_\" + DATA_TYPE + \".pth\"\n",
    "CONFUSION_MATRIX_PATH = OUTPUT_FILE + \"/confusion_matrix_\" + DATA_TYPE + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47205e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_FILE):\n",
    "    os.makedirs(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bc102",
   "metadata": {},
   "source": [
    "### Extract Prepared Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0361b76",
   "metadata": {},
   "source": [
    "#### Vocab Map & Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "file_names = [\"vocab_map\", \"vocab_adj_tf\", \"vocab_adj_npmi\"]\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    datafile = DUMP_DIR + \"/data.%s\" % (file_names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "gcn_vocab_map, gcn_vocab_adj_tf, gcn_vocab_adj_npmi = tuple(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_THRESHOLD > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > TF_THRESHOLD)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if NPMI_THRESHOLD > 0:\n",
    "    gcn_vocab_adj_npmi.data *= (gcn_vocab_adj_npmi.data > NPMI_THRESHOLD)\n",
    "    gcn_vocab_adj_npmi.eliminate_zeros()\n",
    "\n",
    "if VOCAB_ADJ == \"npmi\":\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_npmi]\n",
    "elif VOCAB_ADJ == \"tf\":\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif VOCAB_ADJ == \"all\":\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_npmi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero Ratio for %dth Vocab Adjacency Matrix : %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_vocab_size = len(gcn_vocab_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab486dd",
   "metadata": {},
   "source": [
    "#### Texts & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f74494",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_DIR + \"/processed_train_data.csv\")\n",
    "test_data = pd.read_csv(DATA_DIR + \"/processed_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "test_data.dropna(axis = 0, how = \"any\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf6d33",
   "metadata": {},
   "source": [
    "### Use DataLoader to Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedditDataset(Dataset):\n",
    "    def __init__(self, texts, labels, gcn_vocab_map, gcn_embedding_dim, tokenizer, max_seq_len):\n",
    "        super(FakedditDataset, self).__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.gcn_vocab_map = gcn_vocab_map\n",
    "        self.gcn_embedding_dim = gcn_embedding_dim\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split()\n",
    "        if len(tokens) > self.max_seq_len - 1 - self.gcn_embedding_dim:\n",
    "            tokens = tokens[: self.max_seq_len - 1 - self.gcn_embedding_dim]\n",
    "        \n",
    "        gcn_vocab_ids = []\n",
    "        for t in tokens:\n",
    "            gcn_vocab_ids.append(self.gcn_vocab_map[t])\n",
    "            \n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\" for i in range(self.gcn_embedding_dim + 1)]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        return {\n",
    "            \"input_ids\" : input_ids,\n",
    "            \"attention_mask\" : attention_mask,\n",
    "            \"segment_ids\" : segment_ids,\n",
    "            \"gcn_vocab_ids\" : gcn_vocab_ids,\n",
    "            \"label\" : self.labels[idx],\n",
    "        }\n",
    "    \n",
    "    def pad(self, batch):\n",
    "        gcn_vocab_size=len(self.gcn_vocab_map)\n",
    "        input_len_list = [len(sample[\"input_ids\"]) for sample in batch]\n",
    "        max_input_len = np.array(input_len_list).max()\n",
    "        \n",
    "        f_collect = lambda x: [sample[x] for sample in batch]\n",
    "        f_pad = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]\n",
    "        f_pad2 = lambda x, seqlen: [[-1]+ sample[x] + [-1] * (seqlen - len(sample[x])-1) for sample in batch]\n",
    "        \n",
    "        batch_input_ids = torch.tensor(f_pad(\"input_ids\", max_input_len), dtype = torch.long)\n",
    "        batch_attention_mask = torch.tensor(f_pad(\"attention_mask\", max_input_len), dtype = torch.long)\n",
    "        batch_segment_ids = torch.tensor(f_pad(\"segment_ids\", max_input_len), dtype = torch.long)\n",
    "        batch_label = torch.tensor(f_collect(\"label\"), dtype = torch.long)\n",
    "        batch_gcn_vocab_ids_padded = np.array(f_pad2(\"gcn_vocab_ids\", max_input_len)).reshape(-1)\n",
    "        batch_gcn_swop_eye = torch.eye(gcn_vocab_size + 1)[batch_gcn_vocab_ids_padded][:,:-1]\n",
    "        batch_gcn_swop_eye = batch_gcn_swop_eye.view(len(batch),-1,gcn_vocab_size).transpose(1,2)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\" : batch_input_ids,\n",
    "            \"attention_mask\" : batch_attention_mask,\n",
    "            \"segment_ids\" : batch_segment_ids,\n",
    "            \"gcn_swop_eye\" : batch_gcn_swop_eye,\n",
    "            \"label\" : batch_label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(df, gcn_vocab_map, gcn_embedding_dim, tokenizer, max_seq_len, batch_size):\n",
    "    ds = FakedditDataset(\n",
    "        texts = df[\"cleaned_tokens\"].to_numpy(),\n",
    "        labels = df[\"label\"].to_numpy(),\n",
    "        gcn_vocab_map = gcn_vocab_map,\n",
    "        gcn_embedding_dim = gcn_embedding_dim,\n",
    "        tokenizer = tokenizer,\n",
    "        max_seq_len = max_seq_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size = batch_size, shuffle = False, num_workers = 0, collate_fn = ds.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_data, gcn_vocab_map, GCN_EMBEDDING_DIM, tokenizer, MAX_SEQ_LENGTH, BATCH_SIZE)\n",
    "test_dataloader = create_dataloader(test_data, gcn_vocab_map, GCN_EMBEDDING_DIM, tokenizer, MAX_SEQ_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11647d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b57c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_data[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492c705",
   "metadata": {},
   "source": [
    "### Define Training and Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340c09c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = VGCN_BERT.from_pretrained(PRE_TRAINED_MODEL_NAME, gcn_adj_dim = gcn_vocab_size, gcn_adj_num = len(gcn_adj_list), \n",
    "                                  gcn_embedding_dim = GCN_EMBEDDING_DIM, num_labels = int(DATASET_NUM_CLASSES))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2192fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = L2_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(model, gcn_adj_list, dataloader, isTraining = True):\n",
    "    model.eval()\n",
    "    all_predicts = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = {\"input_ids\": data[\"input_ids\"].to(device), \n",
    "                      \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "                      \"segment_ids\": data[\"segment_ids\"].to(device),\n",
    "                      \"gcn_swop_eye\": data[\"gcn_swop_eye\"].to(device),\n",
    "                      \"label\": data[\"label\"].to(device)\n",
    "                     }\n",
    "\n",
    "            _, logits = model(gcn_adj_list, inputs[\"gcn_swop_eye\"], inputs[\"input_ids\"], \n",
    "                              inputs[\"segment_ids\"], inputs[\"attention_mask\"])\n",
    "\n",
    "            loss = F.cross_entropy(logits.view(-1, int(DATASET_NUM_CLASSES)), inputs[\"label\"])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            all_predicts.extend(predicted.tolist())\n",
    "            all_labels.extend(inputs[\"label\"].tolist())\n",
    "\n",
    "    f1_metrics = f1_score(np.array(all_labels).reshape(-1), np.array(all_predicts).reshape(-1), average='weighted')\n",
    "    print(\"Evaluation Report:\\n\" + classification_report(np.array(all_labels).reshape(-1),\n",
    "              np.array(all_predicts).reshape(-1), digits = 5))\n",
    "    \n",
    "    if not isTraining:\n",
    "        ConfusionMatrixDisplay.from_predictions(all_labels, all_predicts, cmap = \"GnBu\")\n",
    "        plt.savefig(CONFUSION_MATRIX_PATH)\n",
    "        plt.show()\n",
    "        \n",
    "    return total_loss, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0961536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, gcn_adj_list, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        inputs = {\"input_ids\": data[\"input_ids\"].to(device), \n",
    "          \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "          \"segment_ids\": data[\"segment_ids\"].to(device),\n",
    "          \"gcn_swop_eye\": data[\"gcn_swop_eye\"].to(device),\n",
    "          \"label\": data[\"label\"].to(device)\n",
    "         }\n",
    "\n",
    "        _, logits = model(gcn_adj_list, inputs[\"gcn_swop_eye\"], inputs[\"input_ids\"], \n",
    "                          inputs[\"segment_ids\"], inputs[\"attention_mask\"])\n",
    "        loss = F.cross_entropy(logits, inputs[\"label\"])\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 40 == 0:\n",
    "            print(\"Epoch:{}-{}/{}, Train Loss: {}, Cumulated Time: {}m \".format(epoch, idx,\n",
    "                  len(train_dataloader), loss.item(), (time.time() - train_start)/60.0))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "perform_metrics_best = 0\n",
    "\n",
    "for epoch in range(TRAIN_EPOCH):\n",
    "    train_loss = train_epoch(model, gcn_adj_list, train_dataloader, optimizer, device, epoch)\n",
    "\n",
    "    print('*' * 50)\n",
    "    test_loss, curr_metrics= evaluate(model, gcn_adj_list, test_dataloader)\n",
    "    \n",
    "    if (curr_metrics > perform_metrics_best):\n",
    "        perform_metrics_best = curr_metrics\n",
    "        torch.save(model, MODEL_FILE)\n",
    "        \n",
    "    print('*' * 50)\n",
    "    print(\"Epoch:{} Completed, Total Train Loss:{}, Test Loss:{}, Spend {}m \".format(\n",
    "        epoch, train_loss, test_loss, (time.time() - train_start) / 60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_model = torch.load(MODEL_FILE, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b80fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(e_model, gcn_adj_list, test_dataloader, isTraining = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b65a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumour",
   "language": "python",
   "name": "rumour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253.886px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
