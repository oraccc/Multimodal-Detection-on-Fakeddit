{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 17:11:26.495248: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 17:11:28.135728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/oracle/12.2/client64/lib/lib:/usr/local/lib::.\n",
      "2023-04-23 17:11:28.136411: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/oracle/12.2/client64/lib/lib:/usr/local/lib::.\n",
      "2023-04-23 17:11:28.136422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from utils import *\n",
    "from model_vgcn_bert import VGCN_BERT\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Config\n",
    "DATASET_NUM_CLASSES = \"3\"\n",
    "DATASET_SIZE = \"small\"\n",
    "DATA_TYPE = \"text_comments\"\n",
    "DATA_DIR = \"./processed_data/\" + DATASET_NUM_CLASSES + \"_\" + DATASET_SIZE + \"/\" + DATA_TYPE\n",
    "DUMP_DIR = DATA_DIR + \"/dumped_data\"\n",
    "\n",
    "# BERT Config\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "\n",
    "# Model Config\n",
    "GCN_EMBEDDING_DIM = 16\n",
    "LEARNING_RATE = 1e-5\n",
    "L2_DECAY = 0.01\n",
    "VOCAB_ADJ = \"npmi\" # npmi / tf / all\n",
    "NPMI_THRESHOLD = 0.1\n",
    "TF_THRESHOLD = 0.1\n",
    "MAX_SEQ_LENGTH = 450 + GCN_EMBEDDING_DIM\n",
    "TRAIN_EPOCH = 5\n",
    "BATCH_SIZE = 8\n",
    "OUTPUT_FILE = \"./trained_models/\" + DATASET_NUM_CLASSES + \"_\" + DATASET_SIZE\n",
    "MODEL_FILE = OUTPUT_FILE + \"/vgcn_bert_models_\" + DATA_TYPE + \".pth\"\n",
    "CONFUSION_MATRIX_PATH = OUTPUT_FILE + \"/confusion_matrix_\" + DATA_TYPE + \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47205e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_FILE):\n",
    "    os.makedirs(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bc102",
   "metadata": {},
   "source": [
    "### Extract Prepared Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0361b76",
   "metadata": {},
   "source": [
    "#### Vocab Map & Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absolute-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "file_names = [\"vocab_map\", \"vocab_adj_tf\", \"vocab_adj_npmi\"]\n",
    "\n",
    "for i in range(len(file_names)):\n",
    "    datafile = DUMP_DIR + \"/data.%s\" % (file_names[i])\n",
    "    with open(datafile, 'rb') as f:\n",
    "        objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "gcn_vocab_map, gcn_vocab_adj_tf, gcn_vocab_adj_npmi = tuple(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688f5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_THRESHOLD > 0:\n",
    "    gcn_vocab_adj_tf.data *= (gcn_vocab_adj_tf.data > TF_THRESHOLD)\n",
    "    gcn_vocab_adj_tf.eliminate_zeros()\n",
    "if NPMI_THRESHOLD > 0:\n",
    "    gcn_vocab_adj_npmi.data *= (gcn_vocab_adj_npmi.data > NPMI_THRESHOLD)\n",
    "    gcn_vocab_adj_npmi.eliminate_zeros()\n",
    "\n",
    "if VOCAB_ADJ == \"npmi\":\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_npmi]\n",
    "elif VOCAB_ADJ == \"tf\":\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf]\n",
    "elif VOCAB_ADJ == \"all\":\n",
    "    gcn_vocab_adj_list = [gcn_vocab_adj_tf, gcn_vocab_adj_npmi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ec85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero Ratio for 0th Vocab Adjacency Matrix : 92.21295569\n"
     ]
    }
   ],
   "source": [
    "norm_gcn_vocab_adj_list = []\n",
    "for i in range(len(gcn_vocab_adj_list)):\n",
    "    adj = gcn_vocab_adj_list[i]\n",
    "\n",
    "    print('Zero Ratio for %dth Vocab Adjacency Matrix : %.8f' %\n",
    "          (i, 100 * (1 - adj.count_nonzero() / (adj.shape[0] * adj.shape[1]))))\n",
    "\n",
    "    adj = normalize_adj(adj)\n",
    "    norm_gcn_vocab_adj_list.append(sparse_scipy2torch(adj.tocoo()).to(device))\n",
    "\n",
    "gcn_adj_list = norm_gcn_vocab_adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5be3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_vocab_size = len(gcn_vocab_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab486dd",
   "metadata": {},
   "source": [
    "#### Texts & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f74494",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_DIR + \"/processed_train_data.csv\")\n",
    "test_data = pd.read_csv(DATA_DIR + \"/processed_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ee47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(axis = 0, how = \"any\", inplace = True)\n",
    "test_data.dropna(axis = 0, how = \"any\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf6d33",
   "metadata": {},
   "source": [
    "### Use DataLoader to Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sublime-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcec9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedditDataset(Dataset):\n",
    "    def __init__(self, texts, labels, gcn_vocab_map, gcn_embedding_dim, tokenizer, max_seq_len):\n",
    "        super(FakedditDataset, self).__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.gcn_vocab_map = gcn_vocab_map\n",
    "        self.gcn_embedding_dim = gcn_embedding_dim\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx].split()\n",
    "        if len(tokens) > self.max_seq_len - 1 - self.gcn_embedding_dim:\n",
    "            tokens = tokens[: self.max_seq_len - 1 - self.gcn_embedding_dim]\n",
    "        \n",
    "        gcn_vocab_ids = []\n",
    "        for t in tokens:\n",
    "            gcn_vocab_ids.append(self.gcn_vocab_map[t])\n",
    "            \n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\" for i in range(self.gcn_embedding_dim + 1)]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        return {\n",
    "            \"input_ids\" : input_ids,\n",
    "            \"attention_mask\" : attention_mask,\n",
    "            \"segment_ids\" : segment_ids,\n",
    "            \"gcn_vocab_ids\" : gcn_vocab_ids,\n",
    "            \"label\" : self.labels[idx],\n",
    "        }\n",
    "    \n",
    "    def pad(self, batch):\n",
    "        gcn_vocab_size=len(self.gcn_vocab_map)\n",
    "        input_len_list = [len(sample[\"input_ids\"]) for sample in batch]\n",
    "        max_input_len = np.array(input_len_list).max()\n",
    "        \n",
    "        f_collect = lambda x: [sample[x] for sample in batch]\n",
    "        f_pad = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch]\n",
    "        f_pad2 = lambda x, seqlen: [[-1]+ sample[x] + [-1] * (seqlen - len(sample[x])-1) for sample in batch]\n",
    "        \n",
    "        batch_input_ids = torch.tensor(f_pad(\"input_ids\", max_input_len), dtype = torch.long)\n",
    "        batch_attention_mask = torch.tensor(f_pad(\"attention_mask\", max_input_len), dtype = torch.long)\n",
    "        batch_segment_ids = torch.tensor(f_pad(\"segment_ids\", max_input_len), dtype = torch.long)\n",
    "        batch_label = torch.tensor(f_collect(\"label\"), dtype = torch.long)\n",
    "        batch_gcn_vocab_ids_padded = np.array(f_pad2(\"gcn_vocab_ids\", max_input_len)).reshape(-1)\n",
    "        batch_gcn_swop_eye = torch.eye(gcn_vocab_size + 1)[batch_gcn_vocab_ids_padded][:,:-1]\n",
    "        batch_gcn_swop_eye = batch_gcn_swop_eye.view(len(batch),-1,gcn_vocab_size).transpose(1,2)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\" : batch_input_ids,\n",
    "            \"attention_mask\" : batch_attention_mask,\n",
    "            \"segment_ids\" : batch_segment_ids,\n",
    "            \"gcn_swop_eye\" : batch_gcn_swop_eye,\n",
    "            \"label\" : batch_label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41ed9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(df, gcn_vocab_map, gcn_embedding_dim, tokenizer, max_seq_len, batch_size):\n",
    "    ds = FakedditDataset(\n",
    "        texts = df[\"cleaned_tokens\"].to_numpy(),\n",
    "        labels = df[\"label\"].to_numpy(),\n",
    "        gcn_vocab_map = gcn_vocab_map,\n",
    "        gcn_embedding_dim = gcn_embedding_dim,\n",
    "        tokenizer = tokenizer,\n",
    "        max_seq_len = max_seq_len\n",
    "    )\n",
    "    return DataLoader(ds, batch_size = batch_size, shuffle = False, num_workers = 0, collate_fn = ds.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace2f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_data, gcn_vocab_map, GCN_EMBEDDING_DIM, tokenizer, MAX_SEQ_LENGTH, BATCH_SIZE)\n",
    "test_dataloader = create_dataloader(test_data, gcn_vocab_map, GCN_EMBEDDING_DIM, tokenizer, MAX_SEQ_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11647d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b57c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 467])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492c705",
   "metadata": {},
   "source": [
    "### Define Training and Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "340c09c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGCN_BERT(\n",
       "  (embeddings): VGCNBertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (vocab_gcn): VocabGraphConvolution(\n",
       "      (fc_hc): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (act_func): ReLU()\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGCN_BERT.from_pretrained(PRE_TRAINED_MODEL_NAME, gcn_adj_dim = gcn_vocab_size, gcn_adj_num = len(gcn_adj_list), \n",
    "                                  gcn_embedding_dim = GCN_EMBEDDING_DIM, num_labels = int(DATASET_NUM_CLASSES))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2192fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = L2_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "differential-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(model, gcn_adj_list, dataloader, isTraining = True):\n",
    "    model.eval()\n",
    "    all_predicts = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs = {\"input_ids\": data[\"input_ids\"].to(device), \n",
    "                      \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "                      \"segment_ids\": data[\"segment_ids\"].to(device),\n",
    "                      \"gcn_swop_eye\": data[\"gcn_swop_eye\"].to(device),\n",
    "                      \"label\": data[\"label\"].to(device)\n",
    "                     }\n",
    "\n",
    "            _, logits = model(gcn_adj_list, inputs[\"gcn_swop_eye\"], inputs[\"input_ids\"], \n",
    "                              inputs[\"segment_ids\"], inputs[\"attention_mask\"])\n",
    "\n",
    "            loss = F.cross_entropy(logits.view(-1, int(DATASET_NUM_CLASSES)), inputs[\"label\"])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits, -1)\n",
    "            all_predicts.extend(predicted.tolist())\n",
    "            all_labels.extend(inputs[\"label\"].tolist())\n",
    "\n",
    "    f1_metrics = f1_score(np.array(all_labels).reshape(-1), np.array(all_predicts).reshape(-1), average='weighted')\n",
    "    print(\"Evaluation Report:\\n\" + classification_report(np.array(all_labels).reshape(-1),\n",
    "              np.array(all_predicts).reshape(-1), digits = 5))\n",
    "    \n",
    "    if not isTraining:\n",
    "        ConfusionMatrixDisplay.from_predictions(all_labels, all_predicts, cmap = \"GnBu\")\n",
    "        plt.savefig(CONFUSION_MATRIX_PATH)\n",
    "        plt.show()\n",
    "        \n",
    "    return total_loss, f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0961536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, gcn_adj_list, dataloader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        inputs = {\"input_ids\": data[\"input_ids\"].to(device), \n",
    "          \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "          \"segment_ids\": data[\"segment_ids\"].to(device),\n",
    "          \"gcn_swop_eye\": data[\"gcn_swop_eye\"].to(device),\n",
    "          \"label\": data[\"label\"].to(device)\n",
    "         }\n",
    "\n",
    "        _, logits = model(gcn_adj_list, inputs[\"gcn_swop_eye\"], inputs[\"input_ids\"], \n",
    "                          inputs[\"segment_ids\"], inputs[\"attention_mask\"])\n",
    "        loss = F.cross_entropy(logits, inputs[\"label\"])\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if idx % 40 == 0:\n",
    "            print(\"Epoch:{}-{}/{}, Train Loss: {}, Cumulated Time: {}m \".format(epoch, idx,\n",
    "                  len(train_dataloader), loss.item(), (time.time() - train_start)/60.0))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0-0/2152, Train Loss: 1.0407068729400635, Cumulated Time: 0.043645608425140384m \n",
      "Epoch:0-40/2152, Train Loss: 0.7782796025276184, Cumulated Time: 1.5262845913569132m \n",
      "Epoch:0-80/2152, Train Loss: 0.7041869759559631, Cumulated Time: 2.9910873969395957m \n",
      "Epoch:0-120/2152, Train Loss: 1.0055947303771973, Cumulated Time: 4.463466068108876m \n",
      "Epoch:0-160/2152, Train Loss: 0.4494379460811615, Cumulated Time: 5.904240417480469m \n",
      "Epoch:0-200/2152, Train Loss: 0.550123393535614, Cumulated Time: 7.354006246725718m \n",
      "Epoch:0-240/2152, Train Loss: 0.2801685333251953, Cumulated Time: 8.77365856965383m \n",
      "Epoch:0-280/2152, Train Loss: 0.3546294569969177, Cumulated Time: 10.205514502525329m \n",
      "Epoch:0-320/2152, Train Loss: 0.5370128750801086, Cumulated Time: 11.69358864625295m \n",
      "Epoch:0-360/2152, Train Loss: 0.42022159695625305, Cumulated Time: 13.15066343943278m \n",
      "Epoch:0-400/2152, Train Loss: 0.264009565114975, Cumulated Time: 14.628824810187021m \n",
      "Epoch:0-440/2152, Train Loss: 0.6001460552215576, Cumulated Time: 16.13611783583959m \n",
      "Epoch:0-480/2152, Train Loss: 0.19192039966583252, Cumulated Time: 17.65184283653895m \n",
      "Epoch:0-520/2152, Train Loss: 0.08444065600633621, Cumulated Time: 19.152125386397042m \n",
      "Epoch:0-560/2152, Train Loss: 0.18347923457622528, Cumulated Time: 20.613200648625693m \n",
      "Epoch:0-600/2152, Train Loss: 0.6857000589370728, Cumulated Time: 22.056322701772054m \n",
      "Epoch:0-640/2152, Train Loss: 0.2523325979709625, Cumulated Time: 23.774277631441752m \n",
      "Epoch:0-680/2152, Train Loss: 0.27299031615257263, Cumulated Time: 25.418508271376293m \n",
      "Epoch:0-720/2152, Train Loss: 0.19790542125701904, Cumulated Time: 26.9011229634285m \n",
      "Epoch:0-760/2152, Train Loss: 0.4478795528411865, Cumulated Time: 28.473493421077727m \n",
      "Epoch:0-800/2152, Train Loss: 0.12396584451198578, Cumulated Time: 30.242928008238476m \n",
      "Epoch:0-840/2152, Train Loss: 0.10270889848470688, Cumulated Time: 32.04858591556549m \n",
      "Epoch:0-880/2152, Train Loss: 0.23170621693134308, Cumulated Time: 33.818915303548174m \n",
      "Epoch:0-920/2152, Train Loss: 0.13068096339702606, Cumulated Time: 35.64827121098836m \n",
      "Epoch:0-960/2152, Train Loss: 0.2239460051059723, Cumulated Time: 37.37891687949499m \n",
      "Epoch:0-1000/2152, Train Loss: 0.27585476636886597, Cumulated Time: 39.13581286668777m \n",
      "Epoch:0-1040/2152, Train Loss: 0.9497272968292236, Cumulated Time: 40.714611260096234m \n",
      "Epoch:0-1080/2152, Train Loss: 0.956046462059021, Cumulated Time: 42.263529221216835m \n",
      "Epoch:0-1120/2152, Train Loss: 0.4938582181930542, Cumulated Time: 44.09076548020045m \n",
      "Epoch:0-1160/2152, Train Loss: 0.3024778962135315, Cumulated Time: 45.834353943665825m \n",
      "Epoch:0-1200/2152, Train Loss: 0.6622005105018616, Cumulated Time: 48.2595064441363m \n",
      "Epoch:0-1240/2152, Train Loss: 0.3179346024990082, Cumulated Time: 50.087145539124805m \n",
      "Epoch:0-1280/2152, Train Loss: 0.1945127248764038, Cumulated Time: 54.100648907820386m \n",
      "Epoch:0-1320/2152, Train Loss: 0.5196874737739563, Cumulated Time: 57.99935410022736m \n",
      "Epoch:0-1360/2152, Train Loss: 0.16316434741020203, Cumulated Time: 60.47378906408946m \n",
      "Epoch:0-1400/2152, Train Loss: 0.3134501576423645, Cumulated Time: 62.296004275480904m \n",
      "Epoch:0-1440/2152, Train Loss: 0.3497874140739441, Cumulated Time: 65.55192141532898m \n",
      "Epoch:0-1480/2152, Train Loss: 0.3195571005344391, Cumulated Time: 69.69147882064183m \n",
      "Epoch:0-1520/2152, Train Loss: 0.30461835861206055, Cumulated Time: 73.41131157875061m \n",
      "Epoch:0-1560/2152, Train Loss: 0.30653515458106995, Cumulated Time: 77.3450427889824m \n",
      "Epoch:0-1600/2152, Train Loss: 0.32723063230514526, Cumulated Time: 81.39830282131831m \n",
      "Epoch:0-1640/2152, Train Loss: 0.5628883242607117, Cumulated Time: 85.31403314272562m \n",
      "Epoch:0-1680/2152, Train Loss: 0.5180688500404358, Cumulated Time: 89.13492596149445m \n",
      "Epoch:0-1720/2152, Train Loss: 0.10004954785108566, Cumulated Time: 92.98212132056554m \n",
      "Epoch:0-1760/2152, Train Loss: 0.10782146453857422, Cumulated Time: 96.75226666529973m \n",
      "Epoch:0-1800/2152, Train Loss: 0.12886622548103333, Cumulated Time: 100.55913090308508m \n",
      "Epoch:0-1840/2152, Train Loss: 0.41414129734039307, Cumulated Time: 104.46960078080495m \n",
      "Epoch:0-1880/2152, Train Loss: 0.6708981990814209, Cumulated Time: 108.45251019001007m \n",
      "Epoch:0-1920/2152, Train Loss: 0.44095179438591003, Cumulated Time: 112.38467022975286m \n",
      "Epoch:0-1960/2152, Train Loss: 0.054707951843738556, Cumulated Time: 116.20148822863896m \n",
      "Epoch:0-2000/2152, Train Loss: 0.44669991731643677, Cumulated Time: 120.12156105041504m \n",
      "Epoch:0-2040/2152, Train Loss: 0.48760074377059937, Cumulated Time: 123.98210146427155m \n",
      "Epoch:0-2080/2152, Train Loss: 0.30908524990081787, Cumulated Time: 127.88439445892969m \n",
      "Epoch:0-2120/2152, Train Loss: 0.37407076358795166, Cumulated Time: 131.73030974070232m \n",
      "**************************************************\n",
      "Evaluation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.89895   0.87905   0.88889       587\n",
      "           1    0.94490   0.91957   0.93207       373\n",
      "           2    0.89898   0.92308   0.91087       858\n",
      "\n",
      "    accuracy                        0.90814      1818\n",
      "   macro avg    0.91428   0.90723   0.91061      1818\n",
      "weighted avg    0.90839   0.90814   0.90812      1818\n",
      "\n",
      "**************************************************\n",
      "Epoch:0 Completed, Total Train Loss:797.1503477441147, Test Loss:55.77395797800273, Spend 153.55976922512053m \n",
      "Epoch:1-0/2152, Train Loss: 0.3217807710170746, Cumulated Time: 153.63591798146567m \n",
      "Epoch:1-40/2152, Train Loss: 0.10384605824947357, Cumulated Time: 157.40323623021445m \n",
      "Epoch:1-80/2152, Train Loss: 0.4670886695384979, Cumulated Time: 161.3770542939504m \n",
      "Epoch:1-120/2152, Train Loss: 0.4729398190975189, Cumulated Time: 165.22774375677108m \n",
      "Epoch:1-160/2152, Train Loss: 0.1021575778722763, Cumulated Time: 169.1058450380961m \n"
     ]
    }
   ],
   "source": [
    "train_start = time.time()\n",
    "perform_metrics_best = 0\n",
    "\n",
    "for epoch in range(TRAIN_EPOCH):\n",
    "    train_loss = train_epoch(model, gcn_adj_list, train_dataloader, optimizer, device, epoch)\n",
    "\n",
    "    print('*' * 50)\n",
    "    test_loss, curr_metrics= evaluate(model, gcn_adj_list, test_dataloader)\n",
    "    \n",
    "    if (curr_metrics > perform_metrics_best):\n",
    "        perform_metrics_best = curr_metrics\n",
    "        torch.save(model, MODEL_FILE)\n",
    "        \n",
    "    print('*' * 50)\n",
    "    print(\"Epoch:{} Completed, Total Train Loss:{}, Test Loss:{}, Spend {}m \".format(\n",
    "        epoch, train_loss, test_loss, (time.time() - train_start) / 60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_model = torch.load(MODEL_FILE, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b80fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(e_model, gcn_adj_list, test_dataloader, isTraining = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b65a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumour",
   "language": "python",
   "name": "rumour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253.886px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
