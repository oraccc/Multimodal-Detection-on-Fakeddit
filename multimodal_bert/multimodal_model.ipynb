{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee028a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "import PIL\n",
    "from PIL import Image, ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 180960000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc90816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Config\n",
    "DATASET_NUM_CLASSES = 2\n",
    "DATASET_SIZE = \"small\"\n",
    "DATASET_BASE_DIR = \"../data/processed_data/\" + str(DATASET_NUM_CLASSES) + \"_\" + DATASET_SIZE\n",
    "IMAGE_BASE_DIR = DATASET_BASE_DIR + \"/images/\"\n",
    "DATA_TYPE = \"text_comments\" \n",
    "\n",
    "# BERT Config\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHES = 9\n",
    "FUSION_TYPE = \"add\"  # \"cat\", \"add\", \"max\", \"avg\"\n",
    "SAVE_PATH = \"./trained_models/\" + str(DATASET_NUM_CLASSES) + \"_\" + DATASET_SIZE\n",
    "MODEL_PATH = SAVE_PATH + \"/multimodal_models_\" + DATA_TYPE + \"_\" + FUSION_TYPE + \".pth\"\n",
    "CONFUSION_MATRIX_PATH = SAVE_PATH + \"/confusion_matrix_\" + DATA_TYPE + \"_\" + FUSION_TYPE + \".png\"\n",
    "\n",
    "# seed = 20\n",
    "# torch.manual_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cfa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = pd.read_csv(DATASET_BASE_DIR + \"/train_data.csv\")\n",
    "test_data_raw = pd.read_csv(DATASET_BASE_DIR + \"/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_raw[[\"id\", DATA_TYPE, str(DATASET_NUM_CLASSES) + \"_way_label\"]]\n",
    "test_data = test_data_raw[[\"id\", DATA_TYPE, str(DATASET_NUM_CLASSES) + \"_way_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12482879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns = {DATA_TYPE : \"text\", str(DATASET_NUM_CLASSES) + \"_way_label\" : \"label\"}, inplace = True)\n",
    "test_data.rename(columns = {DATA_TYPE : \"text\", str(DATASET_NUM_CLASSES) + \"_way_label\" : \"label\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakedditMultiModalDataset(Dataset):\n",
    "    def __init__(self, texts, labels, ids, tokenizer, data_type, max_len, transform):\n",
    "        super(FakedditMultiModalDataset, self).__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = transform\n",
    "        self.images = [IMAGE_BASE_DIR + data_type + \"/\" + each_id + \".jpg\" for each_id in self.ids]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(text, max_length = self.max_len, padding = \"max_length\",\n",
    "                                              truncation=True, return_tensors=\"pt\")\n",
    "        image = Image.open(self.images[idx])\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype = torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"image\": image,\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a01cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(df, tokenizer, data_type, max_len, transform, batch_size):\n",
    "    ds = FakedditMultiModalDataset(\n",
    "        texts = df.text.to_numpy(),\n",
    "        labels = df.label.to_numpy(),\n",
    "        ids = df.id.to_numpy(),\n",
    "        tokenizer = tokenizer,\n",
    "        data_type = data_type,\n",
    "        max_len = max_len,\n",
    "        transform = transform\n",
    "    )\n",
    "    \n",
    "    return DataLoader(ds, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ad44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\":\n",
    "        transforms.Compose([transforms.Resize([224, 224]),\n",
    "                            transforms.ToTensor()]),\n",
    "    \"test\":\n",
    "        transforms.Compose([transforms.Resize([224, 224]),\n",
    "                           transforms.ToTensor()])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ae3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_data, tokenizer, \"train\", MAX_LEN, data_transforms[\"train\"], BATCH_SIZE)\n",
    "test_dataloader = create_dataloader(test_data, tokenizer, \"test\", MAX_LEN, data_transforms[\"test\"], BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc6141",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = sample_data[\"image\"][0].squeeze()\n",
    "pic = pic.permute((1,2,0)).numpy()\n",
    "plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalForClassification(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_type):\n",
    "        super(MultiModalForClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p = 0.3)\n",
    "        self.image_model = models.resnet50(pretrained = True)\n",
    "        self.fusion_type = fusion_type\n",
    "        in_features = self.image_model.fc.in_features\n",
    "        self.image_model.fc = nn.Linear(in_features, self.bert.config.hidden_size)\n",
    "        \n",
    "        if self.fusion_type == \"cat\":\n",
    "            self.fc = nn.Linear(self.bert.config.hidden_size * 2, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask = None, image = None):\n",
    "        bert_outputs = self.bert(input_ids, attention_mask = attention_mask)\n",
    "        image_outputs = self.image_model(image)\n",
    "        if self.fusion_type == \"cat\":\n",
    "            fused_outputs = torch.cat([bert_outputs[1], image_outputs], dim = 1)\n",
    "        elif self.fusion_type == \"add\":\n",
    "            fused_outputs = bert_outputs[1] + image_outputs\n",
    "        pooled_outputs = self.drop(fused_outputs)\n",
    "        out = self.fc(pooled_outputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7deac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, device, n_examples):\n",
    "    model = model.train()\n",
    "    total_losses = 0\n",
    "    correct_count = 0\n",
    "    progress_bar = tqdm(enumerate(data_loader), total = len(data_loader))\n",
    "    for idx, data in progress_bar:\n",
    "        inputs = {\n",
    "            \"input_ids\": data[\"input_ids\"].to(device), \n",
    "            \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "            \"image\": data[\"image\"].to(device)\n",
    "        }\n",
    "        targets = data[\"label\"].to(device)\n",
    "        outputs = model(**inputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_count += torch.sum(preds == targets)\n",
    "        loss.backward()\n",
    "        total_losses += loss.item()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.set_description(f\"loss:{loss.item():.4f}\")\n",
    "\n",
    "    return correct_count.double() / n_examples, total_losses / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(model, data_loader, device, phase = \"train\"):\n",
    "    model = model.eval()\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for idx, data in progress_bar:\n",
    "            inputs = {\n",
    "                \"input_ids\": data[\"input_ids\"].to(device), \n",
    "                \"attention_mask\": data[\"attention_mask\"].to(device),\n",
    "                \"image\": data[\"image\"].to(device)\n",
    "            }\n",
    "            targets = data[\"label\"].to(device)\n",
    "            probs = model(**inputs)\n",
    "            _, preds = torch.max(probs, dim = 1)\n",
    "            all_targets += targets.cpu().numpy().tolist()\n",
    "            all_predictions += preds.cpu().numpy().tolist()\n",
    "        \n",
    "    f1_metrics = f1_score(np.array(all_targets).reshape(-1), np.array(all_predictions).reshape(-1), average='weighted')\n",
    "    print(classification_report(all_targets, all_predictions, digits = 5))\n",
    "    if phase == \"eval\":\n",
    "        ConfusionMatrixDisplay.from_predictions(all_targets, all_predictions, cmap = \"GnBu\")\n",
    "        plt.savefig(CONFUSION_MATRIX_PATH)\n",
    "        plt.show()\n",
    "        \n",
    "    return f1_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19411998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModalForClassification(DATASET_NUM_CLASSES, FUSION_TYPE)\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, correct_bias = False)\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHES\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a5fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "prev_best = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHES}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, scheduler, device, len(train_data))\n",
    "    print('\\n')\n",
    "    print(f\"Train loss: {train_loss}  Accuracy: {train_acc}\")\n",
    "    test_metrics = eval_model(model, test_dataloader, device)\n",
    "    print(f\"Test F1: {test_metrics}\")\n",
    "    if test_metrics > prev_best:\n",
    "        prev_best = test_metrics\n",
    "        torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(model, test_dataloader, device, phase = \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_model = torch.load(MODEL_PATH, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ab2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_model(e_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ed114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumour",
   "language": "python",
   "name": "rumour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
